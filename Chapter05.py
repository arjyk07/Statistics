#####################################################################

# Chapter 5 정규선형모델
# 파이썬으로 배우는 통계학(update: 2023.01.17)(~5.1 완료)

#####################################################################

"""
    5.1 연속형 독립변수가 하나인 모델(단순회귀)
    5.2 분산분석
    5.3 독립변수가 여럿인 모델
"""

# 에러 제거
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)



# p253  5.1 연속형 독립변수가 하나인 모델(단순회귀)
# p253  5.1.1   분석 준비
# 수치 계산에 사용하는 라이브러리
import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats
# 그래프를 그리기 위한 라이브러리
from matplotlib import pyplot as plt
import seaborn as sns
sns.set()
# 선형모델을 추정하는 라이브러리(경고가 나올 수 있음.)
import statsmodels.formula.api as smf
import statsmodels.api as sm
# 표시 자리수 지정
# %precision 3
# 그래프를 주피터 노트북에 그리기 위한 설정
# %matplotlib inline


# p254  5.1.2   데이터 읽어 들이기와 표시
beer = pd.read_csv("source/sample/5-1-1-beer.csv")
print(beer.head())

# 그래프로 데이터 특징 파악
sns.jointplot(x = "temperature", y = "beer",
              data = beer, color = 'black')


# p255  5.1.3   모델 구축
"""
    맥주 매상 ~ N(β0 + β1 * 기온, σ^2)
    
    모델 구축으로 얻는 장점
    1) 현상 해석 가능 : 계수 β1의 부호를 알면, 기온이 오르면 맥주 매상이 올라갈 지 떨어질 지 판단 가능.
                      계수 β1 ≠ 0 이면, 맥주 매상은 기온 영향을 받는다고 판단 가능.  
                      AIC를 이용한 모델 선택 시, 이 경우에는 맥주 매상을 예측하려면 기온이 필요하다고 해석 가능.
    2) 예측 가능 : 계수 β0, β1, 기온을 알면 맥주 매상 기대값을 계산할 수 있음.
                  기온이 x도 일 때 맥주 매상은 'β0 + β1 * 기온'이 될 것이라는 예측 가능.
"""


# p256  5.1.4   statsmodels를 이용한 모델링
# statsmodels 임포트
import statsmodels.formula.api as smf
# 모델 구축 + 파라미터 추정
lm_model = smf.ols(formula = "beer ~ temperature", data = beer).fit()
"""
    ols : Ordinary Least Squares(범용최소제곱법)의 약자
          모집단분포가 정규분포임을 가정했을 때 최대우도법의 결과는 최소제곱법의 결과와 일치
    formula : 모델 구조 지정
    .fit() 호출까지 잊지 말기!
"""


# p257  5.1.5   추정 결과 표시와 계수 검정
lm_model.summary()
"""
    맥주 매상 ~ N(β0 + β1 * 기온, σ^2)
    
    coef            : 계수값
    std err         : 계수의 표준오차
    t               : t값
    P>|t|           : 귀무가설을 '계수값 = 0'라고 할 때의 p값
    [0.025  0.975]  : 95% 신뢰구간에서 하측신뢰한계와 상측신뢰한계
"""


# p258  5.1.6   summary 함수의 출력 내용 설명
"""
    Dep. Variable : 종속변수의 이름. Dep는 Depended의 약자로, 종속변수라는 의미.
    Model, Method : 범용최소제곱법을 사용했다는 설명
    Date, Time : 모델을 추정한 일시
    No.Observations : 샘플사이즈
    Df Residuals : 샘플사이즈에서 추정된 파라미터 수를 뺀 것
    Df Model : 사용된 독립변수의 수
    Covariance Type : 공분산 타입. 특별히 지정하지 않으면 nonrobust가 된다.
    R-squared, Adj.R-squared : 결정계수와 자유도 조정이 끝난 결정계수
    F-statistic, Prob (F-statistic) : 분산분석 결과
    Log-Likelihood : 최대로그우도
    AIC : 아카이케 정보 기준
    BIC : 베이즈 정보 기준. 정보 기준의 일종이지만 이 책에서는 사용안 함.
"""


# p259  5.1.7   AIC를 이용한 모델 선택
"""
    독립변수가 1개밖에 없기 때문에,
    Null모델의 AIC와 기온이라는 독립변수가 들어간 모델의 AIC 비교
"""
# 독립변수 들어간 모델과 Null 모델
lm_model = smf.ols(formula = "beer ~ temperature", data = beer).fit()
null_model = smf.ols("beer ~ 1", data = beer).fit()

# AIC 비교
lm_model.aic    # 208.90902935575437
null_model.aic  # 227.94194972563105

"""
    AIC = -2 * (최대로그우드 - 추정된 파라미터 수)
"""

# 추정된 모델의 로그우도
lm_model.llf    # -102.45451467787719
# 사용된 독립변수의 수
lm_model.df_model   # 1

# AIC
# 절편(β0)도 추정되었기 때문에 위의 사용된 독립변수의 수에 +1을 하면 추정된 파라미터 수를 구할 수 있음.
-2 * (lm_model.llf - (lm_model.df_model + 1))       # 208.90902935575437
# → AIC는 크고 작음에 의미가 있는 지표로서, AIC의 절대값은 의미가 없다.
#   같은 유형으로 계산된 이상 AIC의 대소 관계는 변하지 않기 때문에 모델 선택에 있어 악영향은 없다.
#   다만, 다른 소프트웨어나 라이브러리에서 계산된 AIC와의 비교는 피해야 한다.


# p261  5.1.8   회귀직선
"""
    회귀직선 : 모델에 의한 종속변수의 추측값을 직선으로 표시한 것.

    종속변수가 연속형 변수일 경우에는 전통적으로 회귀라고 부르기 때문에 회귀라는 이름을 사용했음.
    비선형모델의 경우에는 회귀곡선이라고 부름
"""


# p261  5.1.9   seaborn을 이용한 회귀직선 그래프 그리기
sns.lmplot(x = "temperature", y = "beer", data = beer,      # 음영 부분 : 회귀직선의 95% 신뢰구간
           scatter_kws  = {"color":"black"},                # 산포도 디자인
           line_kws     = {"color":"black"})                # 회귀직선 디자인


# p262  5.1.10  모델을 이용한 예측
"""
    모델 계수를 추정했으니 이를 사용하여 '예측'을 할 수 있음.
    → 추정된 모델에 predict 함수 적용
    → 파라미터에 아무것도 넘기지 않으면 훈련 데이터를 사용한 값이 그대로 출력됨.
"""
lm_model.predict()

# 기온값을 지정해서 예측(파라미터로 데이터프레임을 넘김)
lm_model.predict(pd.DataFrame({"temperature":[0]}))     # 기온이 0도일 때, 맥주 매상 기대값 = 34.610215

"""
    이번에 추정한 모델은
    
        맥주 매상 ~ N(β0 + β1 * 기온, σ^2)
        ex) 맥주 매상 ~ N(20 + 4*기온, σ^2)       ※ 정규분포를 가정했을 경우
        → 맥주 매상은 평균이 (20 + 4*기온), 분산이 σ^2인 정규분포를 따른다.
        
    모델의 예측값, 즉 정규분포에서의 기대값은 'β0 + β1 * 기온'으로 계산됨.
    그러므로 기온이 0도일 때는 β0과 같아짐.
"""
lm_model.params     # Intercept(β0) : 34.610215      temperature(β1) : 0.765428
lm_model.predict(pd.DataFrame({"temperature":[20]}))    # 기온이 20도일 때, 맥주 매상 기대값 = 49.918767

# 위의 49.918767은 (β0 + β1 * 기온(20도))와 같다.
beta0 = lm_model.params[0]
beta1 = lm_model.params[1]
temperature = 20

beta0 + beta1 * temperature     # 49.918767010950546


# p263  5.1.11  잔차 계산
"""
    모델의 평가 방법을 확인해보자!
    원래는 예측을 하기 전에 모델 평가를 해두면 좋다.
    
    모델 평가는 주로 '잔차'를 체크해서 한다.
    정규선형모델의 경우에는 잔차가 '평균이 0인 정규분포'를 따르는 것이므로
    모델이 그 분포를 따르고 있는지 체크하게 된다.
"""
# 잔차 계산하기
resid = lm_model.resid
resid.head(3)
# 0   -5.001481
# 1    5.554095
# 2   -1.864491
# dtype: float64


# 잔차 직접 계산하기(residuals = y - y^)
# y^ = β0 + β1 * 기온
y_hat = beta0 + beta1 * beer.temperature
y_hat.head(3)
# 잔차 = 실제값 - 예측값
(beer.beer - y_hat).head(3)
# 0   -5.001481
# 1    5.554095
# 2   -1.864491
# dtype: float64


# p265  5.1.12  결정계수
"""
    결정계수 : 가지고 있는 데이터에 대해 모델을 적용했을 때의 적합도를 평가한 지표
              summary 함수의 출력에 있는 R-squared
              
              계산식) y: 종속변수, y^: 모델에 의한 추측치(예측치), μ: y의 평균값
              R^2 = ∑(y^ - μ)^2 / ∑(y - μ)^2
              
              모델에 의한 추측치가 종속변수의 실제값과 일치하면 R^2 = 1
"""
# 결정계수 직접 계산
mu = sp.mean(beer.beer)     # μ : y의 평균값
y = beer.beer               # y : 종속변수
yhat = lm_model.predict()   # y^ : 모델에 의한 추측치(예측치)

sp.sum((yhat - mu)**2) / sp.sum((y - mu)**2)        # 0.5039593230611875

# 결정계수 불러오기
lm_model.rsquared                                   # 0.5039593230611858

"""
    잔차 : residuals = y - y^
          → y = y^ + residuals
          
          결정계수(R-squared)의 분모 ∑(y - μ)^2는 다음과 같이 분해 가능
          ∑(y - μ)^2 = ∑(y^ - μ)^2 + ∑residuals^2
          
          → 종속변수 값의 변동 크기 ∑(y - μ)^2는 
          1) 모델로 설명 가능한 변동(∑(y^ - μ)^2)과
          2) 모델로 설명하지 못하는 변동(잔차제곱합, ∑residuals^2)으로 분해할 수 있다는 것!
          
          → 이 때문에 결졍계수(R-squared)는 
          (전체 변동폭 크기)에 대한 (모델로 설명 가능한 변동폭)의 비율
"""
# 모델로 설명 가능한 변동 + 모델로 설명하지 못하는 잔차제곱합의 합계
sp.sum((yhat - mu)**2) + sp.sum(resid**2)       # 3277.114666666671

# 위 계산 결과(3277.114666666671)는 데이터 전체 변동의 합과 같음
sp.sum((y - mu)**2)                             # 3277.1146666666655

"""
    이런 관계가 있기 때문에 결정계수(R-squared)는 아래와 같이 계산 가능
    R^2 = 1 - (∑residuals^2 / ∑(y - μ)^2)
"""
# 위의 수식으로 계산
1 - (sp.sum(resid**2) / sp.sum((y - mu)**2))    # 0.5039593230611857


# p267  5.1.13  수정된 결정계수
"""
    수정된 결정계수 : 독립변수의 수가 늘어나는 것에 대해 페널티를 적용한 결정계수
    (Adjusted R^2)  독립변수의 수가 늘어나면 결정계수는 큰 값을 가진다.
                    → 결정계수가 높아지면 과학습을 일으키기 때문에 조정이 필요하다.
                    
                    계산식) s: 독립변수의 수
                                ∑residuals^2 / (N - s - 1)
                    R^2 = 1 -   ----------------------------
                                ∑(y - μ)^2 / (N - 1) 
                                
"""
# 수정된 결정계수 직접 계산
n = len(beer.beer)      # n : 샘플사이즈
s = 1                   # s : 독립변수의 수
1 - ( (sp.sum(resid**2) / (n - s - 1))  /
      (sp.sum((y - mu)**2) / (n - 1))  )        # 0.48624358459908523

# 수정된 결정계수 코드값
lm_model.rsquared_adj                           # 0.48624358459908534


# p268  5.1.14  잔차 그래프
"""
    잔차의 특징을 보는 가장 간단한 방법은 잔차의 히스토그램을 보는 것.
    이 히스토그램을 보고 정규분포의 특징을 갖고 있는지 눈으로 확인함.
"""
sns.distplot(resid, color = 'black')
# → 그래프 상으로는 좌우대칭으로 정규분포를 따르는 것처럼 보임

# X축이 적합도, Y축이 잔차인 산포도 그리기
# 이 산포도가 완전 랜덤이며, 상관이 없다는 것을 확인함.
# 매우 큰 잔차가 나오지 않는 것도 확인함.
sns.jointplot(x = lm_model.fittedvalues,
              y = resid,
              joint_kws = {"color": "black"},
              marginal_kws = {"color": "black"} )
# TypeError: jointplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 2 keyword-only arguments) were given
# x =, y = 으로 지정하여 해결


# p269  5.1.15  Q-Q 플롯
"""
    Q-Q 플롯 : 이론 상의 분위점과 실제 데이터의 분위점을 산포도 그래프로 그린 것(Q: Quantile)
              - 이론 상의 분위점 : 정규분포의 퍼센트포인트
              - 실제 데이터의 분위점 : 모든 데이터에 대한 분위점   
              → 이론 상의 분위점과 실제 데이터의 분위점을 구해서 그 둘을 비교하는 것으로
                잔차가 정규분포에 근접하는지 아닌지 시각적으로 판단할 수 있음.
"""
# Q-Q 플롯은 sm.qqplot 함수로 그릴 수 있음
# line = "s" 라고 파라미터를 넘김으로써 잔차가 정규분포를 따르면 이 선상에 위치한다는 기준을 표시하게 함.
fig = sm.qqplot(resid, line = "s")

# Q-Q 플롯 직접 만들기
# 우선 데이터를 앞이 작은 것이 오게 정렬
resid_sort = resid.sort_values()
resid_sort.head()

# 이번 데이터는 샘플사이즈 = 30
# 가장 작은 데이터는 하위 몇 %에 위치할까?
# 1 ÷ 31로 계산하면 됨(1부터 시작한다는 점에 주의!)
1 / 31      # 0.03225806451612903

# 30개 샘플 데이터에 대해 모두 위 계산을 한다.
# 그 결과가 이론 상의 누적확률이 됨.
nobs = len(resid_sort)      # 30
cdf = np.arange(1, nobs + 1) / (nobs + 1)
# np.arange(1, nobs + 1)
# array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
#        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])
# nobs + 1 = 31
cdf
# array([0.03225806, 0.06451613, 0.09677419, 0.12903226, 0.16129032,
#        0.19354839, 0.22580645, 0.25806452, 0.29032258, 0.32258065,
#        0.35483871, 0.38709677, 0.41935484, 0.4516129 , 0.48387097,
#        0.51612903, 0.5483871 , 0.58064516, 0.61290323, 0.64516129,
#        0.67741935, 0.70967742, 0.74193548, 0.77419355, 0.80645161,
#        0.83870968, 0.87096774, 0.90322581, 0.93548387, 0.96774194])

# 이론 상의 분위점은 정규분포의 퍼센트포인트를 사용하면 계산할 수 있다.
ppf = stats.norm.ppf(cdf)
ppf
# array([-1.84859629, -1.51792916, -1.30015343, -1.13097761, -0.98916863,
#        -0.86489436, -0.75272879, -0.64932391, -0.55244258, -0.46049454,
#        -0.37228936, -0.28689392, -0.20354423, -0.12158738, -0.04044051,
#         0.04044051,  0.12158738,  0.20354423,  0.28689392,  0.37228936,
#         0.46049454,  0.55244258,  0.64932391,  0.75272879,  0.86489436,
#         0.98916863,  1.13097761,  1.30015343,  1.51792916,  1.84859629])
# → 이제 가로축에 이론 상의 분위점(ppf), 세로축에 정렬된 데이터(resid_sort)를 지정해서
#   산포도를 그리면 Q-Q플롯을 그릴 수 있다.


# p271  5.1.16  summary 함수의 출력으로 보는 잔차 체크
"""
    summary 함수의 추정 결과 결과값의 마지막 3번째 표
    
    Prob(Omnibus), Prob(JB)는 잔차의 정규성에 대한 검정 결과
    - 귀무가설 : 잔차가 정규분포를 따른다.
    - 대립가설 : 잔차가 정규분포와 다르다.
    
    p값이 0.05보다 큰 지 확인한다.
    → 검정의 비대칭성이 있으므로 p값이 0.05보다 크다고 해도 정규분포라고 주장할 수 없다는 점에 주의해야 함.
      이러한 검정은 명확한 문제가 있는지 판단하는 것.
    정규분포와 다른지 여부를 판단할 때 Skew(왜도)와 Kurtosis(첨도)라는 지표 사용.
    - 왜도 : 히스토그램의 좌우비대칭 방향과 그 정도를 측정하는 지표. 
        왜도>0 : 오른쪽 자락이 길어짐 
    - 첨도 : 히스토그램 중심부의 뾰족함을 측정하는 지표. 
        값이 클수록 뾰족해짐.
    - Durbin-Watson : 잔차의 자기상관을 체크하는 지표
        이 지표가 2 전후라면 문제없다고 판단할 수 있음.
        특히, 시계열 데이터를 대상으로 분석하는 경우 반드시 Durbin-Watson이 2 전후임을 확인해야 함.
        잔차에 자기상관이 있으면 계수의 t검정 결과를 신뢰할 수 없게 됨.
        → 이 문제를 '보여주기 위함 회귀'라고 부름.
        Durbin-Watson 통계량이 2보다 크게 차이가 난다면 일반화 제곱법 등의 사용을 검토할 필요가 있음.  
"""






